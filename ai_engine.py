# ai_engine.py - Ù…ÙˆØªÙˆØ± Ø§ØµÙ„ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨Ø§Ù„Ø§
# --------------------------------------------------------------

import json
import os
import re
import hashlib
import threading
import queue
import time
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import gc

# ================ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ ================
class PersianTextProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± ÙÙˆÙ‚ Ø³Ø±ÛŒØ¹ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    
    def __init__(self):
        self.alphabet = set('Ø¢Ø§Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ú˜Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ')
        self.question_words = {'Ú©ÛŒØ³Øª', 'Ú©ÛŒ', 'Ú©Ø¬Ø§Ø³Øª', 'Ú†ÛŒØ³Øª', 'Ú†Ø±Ø§', 'Ú†Ø·ÙˆØ±', 'Ú†Ú¯ÙˆÙ†Ù‡', 'Ú©Ø¯Ø§Ù…', 'Ø¢ÛŒØ§'}
        self.stop_words = {'Ø§Ø³Øª', 'Ø¨ÙˆØ¯', 'Ù‡Ø³Øª', 'Ù…ÛŒ', 'Ú©Ù‡', 'Ø±Ø§', 'Ø¨Ø§', 'Ø§Ø²', 'Ø¨Ù‡', 'Ø¨Ø±Ø§ÛŒ', 'Ùˆ', 'ÛŒØ§'}
        self.cache = {}
        self.cache_size = 10000
        
    def normalize(self, text):
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ú©Ø´"""
        if not text:
            return ""
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        text = text.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©')
        text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
        if len(self.cache) < self.cache_size:
            self.cache[text_hash] = text
        
        return text
    
    def tokenize(self, text):
        """ØªØ¬Ø²ÛŒÙ‡ Ø³Ø±ÛŒØ¹"""
        return text.split()
    
    def extract_features(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ Ù‡Ø²ÛŒÙ†Ù‡"""
        text = self.normalize(text)
        words = self.tokenize(text)
        
        return {
            'word_count': len(words),
            'words': words[:10],  # ÙÙ‚Ø· Û±Û° Ú©Ù„Ù…Ù‡ Ø§ÙˆÙ„
            'has_question': any(w in self.question_words for w in words)
        }

# ================ Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ ================
class SmartCache:
    """Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø­Ø°Ù Ø®ÙˆØ¯Ú©Ø§Ø±"""
    
    def __init__(self, max_size=10000, ttl=3600):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = {}
        self.timestamps = {}
        self.lock = threading.Lock()
    
    def get(self, key):
        with self.lock:
            if key in self.cache:
                if time.time() - self.timestamps[key] < self.ttl:
                    return self.cache[key]
                else:
                    del self.cache[key]
                    del self.timestamps[key]
        return None
    
    def set(self, key, value):
        with self.lock:
            if len(self.cache) >= self.max_size:
                # Ø­Ø°Ù Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ†
                oldest = min(self.timestamps.items(), key=lambda x: x[1])
                del self.cache[oldest[0]]
                del self.timestamps[oldest[0]]
            
            self.cache[key] = value
            self.timestamps[key] = time.time()
    
    def clear(self):
        with self.lock:
            self.cache.clear()
            self.timestamps.clear()

# ================ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ ================
class VectorSearchEngine:
    """Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡"""
    
    def __init__(self):
        self.vectorizer = HashingVectorizer(
            n_features=2**16,  # 65536 ÙˆÛŒÚ˜Ú¯ÛŒ
            ngram_range=(1, 3),
            norm='l2',
            alternate_sign=False
        )
        self.vectors = None
        self.documents = []
        self.doc_ids = []
        self.lock = threading.RLock()
        self.update_queue = queue.Queue()
        self.is_updating = False
    
    def add_documents(self, documents):
        """Ø§ÙØ²ÙˆØ¯Ù† Ø§Ø³Ù†Ø§Ø¯ Ø¬Ø¯ÛŒØ¯"""
        with self.lock:
            start_idx = len(self.documents)
            for doc in documents:
                self.documents.append(doc['question'])
                self.doc_ids.append(doc['id'])
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§
            if len(self.documents) > 0:
                texts = [d['question'] for d in self.documents[start_idx:]]
                new_vectors = self.vectorizer.transform(texts)
                
                if self.vectors is None:
                    self.vectors = new_vectors
                else:
                    from scipy.sparse import vstack
                    self.vectors = vstack([self.vectors, new_vectors])
    
    def search(self, query, top_k=5):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹"""
        with self.lock:
            if self.vectors is None or len(self.documents) == 0:
                return []
            
            query_vector = self.vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.vectors)[0]
            
            # Ú¯Ø±ÙØªÙ† top_k
            top_indices = np.argpartition(similarities, -top_k)[-top_k:]
            results = []
            
            for idx in top_indices:
                if similarities[idx] > 0.1:
                    results.append({
                        'id': self.doc_ids[idx],
                        'score': float(similarities[idx])
                    })
            
            return sorted(results, key=lambda x: x['score'], reverse=True)

# ================ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ================
class KeywordSearchEngine:
    """Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø¹Ú©ÙˆØ³"""
    
    def __init__(self):
        self.inverted_index = defaultdict(set)
        self.documents = {}
        self.lock = threading.RLock()
    
    def add_document(self, doc_id, text):
        """Ø§ÙØ²ÙˆØ¯Ù† Ø³Ù†Ø¯ Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³"""
        with self.lock:
            self.documents[doc_id] = text
            words = set(text.split())
            for word in words:
                self.inverted_index[word].add(doc_id)
    
    def search(self, query, threshold=0.3):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø¹Ú©ÙˆØ³"""
        words = set(query.split())
        if not words:
            return []
        
        scores = defaultdict(float)
        
        for word in words:
            for doc_id in self.inverted_index.get(word, []):
                scores[doc_id] += 1.0
        
        if not scores:
            return []
        
        max_score = max(scores.values())
        results = []
        
        for doc_id, score in scores.items():
            norm_score = score / len(words)
            if norm_score >= threshold:
                results.append({
                    'id': doc_id,
                    'score': norm_score
                })
        
        return sorted(results, key=lambda x: x['score'], reverse=True)

# ================ Ù…ÙˆØªÙˆØ± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ================
class LearningEngine:
    """Ù…ÙˆØªÙˆØ± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ"""
    
    def __init__(self):
        self.vector_engine = VectorSearchEngine()
        self.keyword_engine = KeywordSearchEngine()
        self.processor = PersianTextProcessor()
        self.knowledge_base = []
        self.user_questions = []
        self.learning_stats = {
            'total_learned': 0,
            'total_asked': 0,
            'success_rate': 0
        }
        self.lock = threading.RLock()
        self.cache = SmartCache(max_size=5000, ttl=3600)
        self.executor = ThreadPoolExecutor(max_workers=4)
        
    def learn(self, question, answer, source='manual'):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÛŒÚ© Ø¯Ø§Ù†Ø´ Ø¬Ø¯ÛŒØ¯"""
        with self.lock:
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            question_norm = self.processor.normalize(question)
            
            # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
            for item in self.knowledge_base:
                if item['question'] == question_norm:
                    item['answer'] = answer
                    item['learn_count'] = item.get('learn_count', 1) + 1
                    item['updated'] = datetime.now().isoformat()
                    return True, "Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯"
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ø¬Ø¯ÛŒØ¯
            doc_id = len(self.knowledge_base) + 1
            new_item = {
                'id': doc_id,
                'question': question_norm,
                'answer': answer,
                'source': source,
                'created': datetime.now().isoformat(),
                'updated': datetime.now().isoformat(),
                'used_count': 0,
                'learn_count': 1,
                'success_count': 0
            }
            
            self.knowledge_base.append(new_item)
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
            self.vector_engine.add_documents([new_item])
            self.keyword_engine.add_document(doc_id, question_norm)
            
            self.learning_stats['total_learned'] += 1
            
            return True, "ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯"
    
    def bulk_learn(self, texts, source='bulk'):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú¯Ø±ÙˆÙ‡ÛŒ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ"""
        documents = []
        count = 0
        
        for line in texts.strip().split('\n'):
            if '|' in line:
                parts = line.split('|', 1)
                if len(parts) == 2:
                    q = self.processor.normalize(parts[0].strip())
                    a = parts[1].strip()
                    
                    doc_id = len(self.knowledge_base) + count + 1
                    documents.append({
                        'id': doc_id,
                        'question': q,
                        'answer': a,
                        'source': source
                    })
                    count += 1
        
        # Ø§ÙØ²ÙˆØ¯Ù† ÛŒÚ©Ø¬Ø§
        with self.lock:
            for doc in documents:
                doc['created'] = datetime.now().isoformat()
                doc['used_count'] = 0
                doc['learn_count'] = 1
                self.knowledge_base.append(doc)
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
            self.vector_engine.add_documents(documents)
            for doc in documents:
                self.keyword_engine.add_document(doc['id'], doc['question'])
            
            self.learning_stats['total_learned'] += count
        
        return count, []
    
    def search(self, query, threshold=0.2):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø§ Û³ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…"""
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´
        cache_key = hashlib.md5(query.encode()).hexdigest()
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        query_norm = self.processor.normalize(query)
        results = []
        seen_ids = set()
        
        # Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Û±: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ
        vector_results = self.vector_engine.search(query_norm, top_k=10)
        for r in vector_results:
            if r['id'] not in seen_ids:
                item = self.knowledge_base[r['id'] - 1]
                results.append({
                    'id': r['id'],
                    'answer': item['answer'],
                    'score': r['score'] * 1.2,
                    'method': 'vector'
                })
                seen_ids.add(r['id'])
        
        # Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Û²: Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        keyword_results = self.keyword_engine.search(query_norm, threshold=0.3)
        for r in keyword_results:
            if r['id'] not in seen_ids:
                item = self.knowledge_base[r['id'] - 1]
                results.append({
                    'id': r['id'],
                    'answer': item['answer'],
                    'score': r['score'],
                    'method': 'keyword'
                })
                seen_ids.add(r['id'])
        
        # Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Û³: ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÛŒÙ‚
        for item in self.knowledge_base:
            if item['question'] == query_norm and item['id'] not in seen_ids:
                results.append({
                    'id': item['id'],
                    'answer': item['answer'],
                    'score': 1.0,
                    'method': 'exact'
                })
                seen_ids.add(item['id'])
                break
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        results = sorted(results, key=lambda x: x['score'], reverse=True)[:3]
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
        if results:
            self.cache.set(cache_key, results)
        
        return results
    
    def ask(self, query):
        """Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®"""
        self.learning_stats['total_asked'] += 1
        
        results = self.search(query)
        
        if results:
            best = results[0]
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
            with self.lock:
                item = self.knowledge_base[best['id'] - 1]
                item['used_count'] += 1
                item['success_count'] = item.get('success_count', 0) + 1
                item['last_used'] = datetime.now().isoformat()
            
            return {
                'answer': best['answer'],
                'score': best['score'],
                'method': best['method'],
                'found': True
            }
        
        return {'answer': None, 'found': False}
    
    def record_user_question(self, question):
        """Ø«Ø¨Øª Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡"""
        if len(self.user_questions) > 100000:  # Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û°Û° Ù‡Ø²Ø§Ø±
            self.user_questions = self.user_questions[-50000:]
        
        self.user_questions.append({
            'question': question,
            'time': datetime.now().isoformat(),
            'asked_count': 1
        })
    
    def get_popular_questions(self, limit=100):
        """Ú¯Ø±ÙØªÙ† Ø³ÙˆØ§Ù„Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù†"""
        counter = Counter()
        for q in self.user_questions:
            counter[q['question']] += 1
        
        return counter.most_common(limit)
    
    def get_stats(self):
        """Ø¢Ù…Ø§Ø± Ø³Ø±ÛŒØ¹"""
        return {
            'knowledge_count': len(self.knowledge_base),
            'total_learned': self.learning_stats['total_learned'],
            'total_asked': self.learning_stats['total_asked'],
            'user_questions': len(self.user_questions),
            'cache_size': len(self.cache.cache)
        }

# ================ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ ================
class DatabaseManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø´Ø§Ø±Ø¯ÛŒÙ†Ú¯"""
    
    def __init__(self, base_dir='data'):
        self.base_dir = base_dir
        self.shards = {}
        self.current_shard = 0
        self.max_shard_size = 10000
        os.makedirs(base_dir, exist_ok=True)
        self.load_all()
    
    def get_shard(self, doc_id):
        """ØªØ¹ÛŒÛŒÙ† Ø´Ø§Ø±Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ID"""
        shard_num = doc_id // self.max_shard_size
        return f"shard_{shard_num}.json"
    
    def save_document(self, doc):
        """Ø°Ø®ÛŒØ±Ù‡ Ø³Ù†Ø¯ Ø¯Ø± Ø´Ø§Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨"""
        shard_file = self.get_shard(doc['id'])
        shard_path = os.path.join(self.base_dir, shard_file)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø§Ø±Ø¯
        if shard_file not in self.shards:
            if os.path.exists(shard_path):
                with open(shard_path, 'r', encoding='utf-8') as f:
                    self.shards[shard_file] = json.load(f)
            else:
                self.shards[shard_file] = []
        
        # Ø§ÙØ²ÙˆØ¯Ù† ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
        found = False
        for i, item in enumerate(self.shards[shard_file]):
            if item['id'] == doc['id']:
                self.shards[shard_file][i] = doc
                found = True
                break
        
        if not found:
            self.shards[shard_file].append(doc)
        
        # Ø°Ø®ÛŒØ±Ù‡
        with open(shard_path, 'w', encoding='utf-8') as f:
            json.dump(self.shards[shard_file], f, ensure_ascii=False, indent=2)
    
    def load_all(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù‡Ù…Ù‡ Ø´Ø§Ø±Ø¯Ù‡Ø§"""
        for f in os.listdir(self.base_dir):
            if f.startswith('shard_') and f.endswith('.json'):
                with open(os.path.join(self.base_dir, f), 'r', encoding='utf-8') as f:
                    self.shards[f] = json.load(f)

# ================ Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ================
class ScalablePersianAI:
    """Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ± Ø¨Ø±Ø§ÛŒ Ù…ÛŒÙ„ÛŒÙˆÙ†â€ŒÙ‡Ø§ Ú©Ø§Ø±Ø¨Ø±"""
    
    def __init__(self):
        self.engine = LearningEngine()
        self.db = DatabaseManager()
        self.processor = PersianTextProcessor()
        self.stats = {
            'start_time': time.time(),
            'total_requests': 0,
            'avg_response_time': 0
        }
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ù†Ø´
        self.load_knowledge()
    
    def load_knowledge(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ù†Ø´ Ø§Ø² Ø´Ø§Ø±Ø¯Ù‡Ø§"""
        for shard_file, items in self.db.shards.items():
            for item in items:
                self.engine.knowledge_base.append(item)
                self.engine.keyword_engine.add_document(item['id'], item['question'])
        
        if self.engine.knowledge_base:
            self.engine.vector_engine.add_documents(self.engine.knowledge_base)
        
        print(f"ğŸ“š {len(self.engine.knowledge_base)} Ø¯Ø§Ù†Ø´ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
    
    def ask(self, question):
        """Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø²Ù…Ø§Ù†"""
        start = time.time()
        
        # Ø«Ø¨Øª Ø³ÙˆØ§Ù„
        self.engine.record_user_question(question)
        
        # Ø¬Ø³ØªØ¬Ùˆ
        result = self.engine.ask(question)
        
        # Ø¢Ù…Ø§Ø±
        self.stats['total_requests'] += 1
        response_time = time.time() - start
        self.stats['avg_response_time'] = (
            self.stats['avg_response_time'] * 0.95 + response_time * 0.05
        )
        
        return result
    
    def learn(self, text, source='manual'):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        learned, errors = self.engine.bulk_learn(text, source)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ù†Ø´ Ø¬Ø¯ÛŒØ¯
        for i in range(len(self.engine.knowledge_base) - learned, len(self.engine.knowledge_base)):
            self.db.save_document(self.engine.knowledge_base[i])
        
        return learned, errors
    
    def get_stats(self):
        """Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„"""
        engine_stats = self.engine.get_stats()
        return {
            **engine_stats,
            'uptime': time.time() - self.stats['start_time'],
            'total_requests': self.stats['total_requests'],
            'avg_response_ms': self.stats['avg_response_time'] * 1000,
            'requests_per_second': self.stats['total_requests'] / (time.time() - self.stats['start_time'])
        }
    
    def get_popular_questions(self, limit=10):
        """Ú¯Ø±ÙØªÙ† Ø³ÙˆØ§Ù„Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±"""
        return self.engine.get_popular_questions(limit)
